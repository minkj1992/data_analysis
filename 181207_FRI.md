# Twoway anova


## ex1
rep(replication)
delivery.mod1 = aov(Time ~ Destination*Service, data = delivery.df)

aov(analysis of variance)

Mean sq: variance
residuals mean sq: within variance
PR(>F): 잘못될 확률 0.05보다 작으면 ㄱㅊ


> summary(delivery.mod1)
                    Df  Sum Sq Mean Sq  F value    Pr(>F)    
Destination          4 17.5415  4.3854  61.1553 5.408e-14 ***
Service              2 23.1706 11.5853 161.5599 < 2.2e-16 ***
Destination:Service  8  4.1888  0.5236   7.3018 2.360e-05 ***
Residuals           30  2.1513  0.0717                       
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

-> interaction effect가 존재한다.
이제 어떻게 발현되는지 말해주어야 한다.

> TukeyHSD(delivery.mod1, which = "Service")
  Tukey multiple comparisons of means
    95% family-wise confidence level
 
Fit: aov(formula = Time ~ Destination * Service, data = delivery.df)
 
$Service
                        diff        lwr       upr     p adj
Carrier 2-Carrier 1 1.498667  1.2576092 1.7397241 0.0000000
Carrier 3-Carrier 1 1.544667  1.3036092 1.7857241 0.0000000
Carrier 3-Carrier 2 0.046000 -0.1950575 0.2870575 0.8856246

> carrier2&3은 diff가 없다. (1과 2&3은 차이가 있다.)

> ggpubr을 통해 그래프를 그려볼 수 있다.


## ex2 예제

length(plant.df$weight): 변수들의 갯수

## ex3 예제
아침 저녁에 어느 시간데에, 운동화 종류에 따라서 basketball 차이 확인

> attach(basketball)
basketball$made~ Time+shoes이렇게 쓸필요없이 -> aov(Made~Time+Shoes) 하면된다.

+를 하였으니, interaction effect는 무시.
            Df Sum Sq Mean Sq F value Pr(>F)
Time         1   7.56    7.56   0.349  0.565
Shoes        1  39.06   39.06   1.802  0.202
Residuals   13 281.81   21.68   


*를 하면 interaction effect
> b.model2 <- aov(Made~Time*Shoes)
> summary(b.model2)
            Df Sum Sq Mean Sq F value Pr(>F)
Time         1   7.56    7.56   0.344  0.568
Shoes        1  39.06   39.06   1.777  0.207
Time:Shoes   1  18.06   18.06   0.822  0.382
Residuals   12 263.75   21.98     


## ex5 stress management
medical: 약, physical:육체적 치료

$Treatment
                 diff        lwr       upr     p adj
mental-medical      2  0.7968988 3.2031012 0.0013531
physical-medical    1 -0.2031012 2.2031012 0.1135025
physical-mental    -1 -2.2031012 0.2031012 0.1135025


mental-medical은 차이가 있고 나머지는 없다.
그렇다면  mental vs mediacal?

attatch(stressdta)
tapply(stressreduction~treatement,data=stressdata,mean)
mental	medaical	physical
4	6	5

std를 같이 구해주어 tapply를 통해 구해준 평균값을 보고서에 같이 넣어주면 된다


#######################################

#Simple Linear Regression

ss res = error 제곱의합

Multiple R-squared:  0.5041,	Adjusted R-squared:  0.4987 에서 sqrt씌우면 r(corelation)값이 된다.
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  32.6267     1.1439  28.523  < 2e-16 ***
EngineSize   -3.8464     0.3999  -9.618 1.61e-15 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.979 on 91 degrees of freedom
Multiple R-squared:  0.5041,	Adjusted R-squared:  0.4987 
F-statistic: 92.51 on 1 and 91 DF,  p-value: 1.606e-15


 -9.618 을 제곱하면 f값나온다.
***********
Does the model fit the data well?

Plot the residuals and check the regression diagnostics.
see visualizing residuals

만약 mode$residual: 각 case별 residual(실측치와의 차이)

> plot(mod$residuals) 하면 mod데이터의  residual이 그려진다. error분포는 무작위여야 한다. 패턴이 있으면 regression을 잘못한 것이다.

#E.g.1

Residual standard error: 3049 on 27 degrees of freedom
Multiple R-squared:  0.1531,	Adjusted R-squared:  0.1218 
F-statistic: 4.883(약 5) on 1 and 27 DF,  p-value: 0.03579(0.05보다 작다)

beta coefficient(표준화, 단위의 차이 등)
:독립변인이 하나가 아닌 여러개 있을때, beta값으로 바꾸면 그 두 크기의 절대값을 비교하면 누가 더 영향 있는지 비교 가능하다.

###################
c.f

zero ordered cor
semi partial cor
partial cor(분모가 제일작음)


#multiple regression

Call:
lm(formula = Cars93$MPG.city ~ Cars93$EngineSize + Cars93$Price)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.4511 -2.0637 -0.5156  1.6239 16.9372 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       33.34647    1.12251  29.707  < 2e-16 ***
Cars93$EngineSize -2.98885    0.47808  -6.252 1.33e-08 ***
Cars93$Price      -0.15415    0.05134  -3.002  0.00347 ** 
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.815 on 90 degrees of freedom
Multiple R-squared:  0.5492,	Adjusted R-squared:  0.5392 
F-statistic: 54.83 on 2 and 90 DF,  p-value: 2.674e-16


Questions:

What is R2?  r^2 = reg/total

How many cars are involved in this test? (cf. df = 90)
If I eliminate the R2 from the above output, can you still identify what it is?



***
in the meantime,
```R
> lm.beta(lm.model)
Cars93$EngineSize      Cars93$Price 
       -0.5517121        -0.2649553 
> cor(MPG.city,EngineSize)
[1] -0.7100032
> cor(EngineSize,Price)
[1] 0.5974254
> cor(MPG.city,Price)
[1] -0.5945622
> 
```


plot(lm.model$residuals)
abline(h=0, col="red")

>residual과 실측치 같을때 의 선을 그려준다.



